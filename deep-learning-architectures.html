<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>deep-learning-architectures</title>
<!-- 2019-01-23 Wed 17:00 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="Re-imaging" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">deep-learning-architectures</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. Deep Learning Architecture Comparison</a>
<ul>
<li><a href="#sec-1-1">1.1. Motivation and Aims</a></li>
<li><a href="#sec-1-2">1.2. Dataset</a></li>
<li><a href="#sec-1-3">1.3. Latest in Generative Models</a></li>
<li><a href="#sec-1-4">1.4. Architectures</a>
<ul>
<li><a href="#sec-1-4-1">1.4.1. General Models</a></li>
<li><a href="#sec-1-4-2">1.4.2. Discrimination and Classification</a></li>
<li><a href="#sec-1-4-3">1.4.3. Memory Networks</a></li>
<li><a href="#sec-1-4-4">1.4.4. Image Generation</a></li>
</ul>
</li>
<li><a href="#sec-1-5">1.5. Implementations</a>
<ul>
<li><a href="#sec-1-5-1">1.5.1. Deep Convolutional Neural Networks (classification)</a></li>
<li><a href="#sec-1-5-2">1.5.2. Generative Adversarial Networks</a></li>
<li><a href="#sec-1-5-3">1.5.3. Variational Auto-Encoders (VAE)</a></li>
</ul>
</li>
<li><a href="#sec-1-6">1.6. Conclusion</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Deep Learning Architecture Comparison</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> Motivation and Aims</h3>
<div class="outline-text-3" id="text-1-1">
<p>
This brief survey has been undertaken in order to establish a general scope of practices in the field of Deep Learning architectures. It is particularly focused on deep learning,  primarily those that have been successfully used towards image or text generation. Deep Learning is here defined as any machine learning task that utilises multi-layer neural networks.
</p>

<p>
The goal of the larger project is to generate images (figures) and text using the arXiv database in order to explore the use of machine learning and statistical methods on this data to generate new models. We will also attempt to generate text using text data.
</p>

<p>
The aim here is to provide a brief overview comparing particular architectures at the highest level, and then discuss some specific implementations. This overview should help provide an overview of current practices in deep learning, especially generative models. It should also reduce the time spent exploring particular architectures in terms of experimentation and training time.
</p>
</div>
</div>
<div id="outline-container-sec-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> Dataset</h3>
<div class="outline-text-3" id="text-1-2">
<p>
The arXiv dataset consists of approximately 1.5 million e-prints in Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering and Systems Science, and Economics [ref: <a href="https://arxiv.org/">https://arxiv.org/</a>]. These are downloadable via Amazon S3 as either PDFs or source data. The source data generally consists of LaTex files with accompanying images that are used to generate the PDFs. Some of the texts/authors do not provide their source and are only downloadable as PDF files.
</p>

<p>
We have also downloaded the associated metadata for the entire database in the form of individual xml files for each e-print. This metadata contains the title, authors, abstract, field and category of research, and date of publication.
</p>

<p>
Each e-print is formatted within an expected framework for the structuring of academic scientific and mathematical texts. The text components are generally divided into title, authors, abstract, and body text; with the text generally conforming to a structure of introduction, methods, results, and conclusion. Within these e-prints there are a number of images or figures that are typically given a figure number and caption. The figure number, caption, location within the paper (i.e. which section of the structure) and associated metadata of the e-print could be used as labels for the images. Image data could also be used as unlabelled training data, and some models may be able to 'learn' various features of the data in order to interpolate through them.
</p>

<p>
This document covers both supervised and unsupervised learning options, as we have not yet determined the use and labelling of data. There is also the option of using semi-supervised training or of exploring the latent space of the distribution in an attempt to move between different labels (such as field of research, or other potential labels).
</p>

<p>
The images found in papers generally relate to science, mathematics, and computing, and therefore often take the form of graphs, equations, or figures, as well as some photographic images such as examples of image classification/generation. It seems that being able to render high detail images (not necessarily high resolution) that are clear and sharp would give the best results for this particular dataset.
</p>

<p>
Note: Generative Adversarial Networks and other similar models are typically considered to be unsupervised learning algorithms. They do not use labels in their training data. While training, these models label images as either generated or from the dataset (fake or real), which is used with the classifier to more accurately model the data (i.e. density estimation).
</p>
</div>
</div>
<div id="outline-container-sec-1-3" class="outline-3">
<h3 id="sec-1-3"><span class="section-number-3">1.3</span> Latest in Generative Models</h3>
<div class="outline-text-3" id="text-1-3">
<p>
It appears that the state of the art in image generation continues to be found in Generative Adversarial Networks (GANs) and some of the more interesting recent developments are coming from the NVidia research group. GANs primarily operate on unlabelled training data and instead attempt to create images that fit within the probability distribution of the training data.
</p>

<p>
The latest NVidia projects have been working with large datasets of facial images (celebrity images from the soon-to-be-released FFHQ dataset), as well as CIFAR and LSUN. They use different techniques to generate ways of moving through the 'feature space' and interpolating through latent space and between different images that are still close to the patterns of the dataset. The latest paper (Dec 2018) uses a style transfer approach in conjunction with progressively trained GANs to be able to interpolate smoothly between images. See StyleGAN video <a href="https://www.youtube.com/watch?time_continue=3&v=kSLJriaOumA">https://www.youtube.com/watch?time_continue=3&v=kSLJriaOumA</a> and paper <a href="https://arxiv.org/abs/1812.04948">https://arxiv.org/abs/1812.04948</a> (and related Progressive GANs <a href="https://arxiv.org/abs/1710.10196">https://arxiv.org/abs/1710.10196</a>). (Interestingly they also do a fair bit of post-processing of their "results" images). 
</p>

<p>
The other recent image generation models of interest are the BigGAN <a href="https://arxiv.org/abs/1809.11096">https://arxiv.org/abs/1809.11096</a> from Google and the StackGAN <a href="https://arxiv.org/abs/1710.10916">https://arxiv.org/abs/1710.10916</a>. There are also a few non-GAN approaches such as Variational Auto-Encoders, Flow-based Deep Generative Models, and De-Convolutional Inverse Graphics Networks but they aren't discussed as much as GANs at the moment and don't seem to get the same results in terms of image quality, but might be worth investigating because they are (at times) quicker and simpler to train and can produce solid results.
</p>

<p>
There are a huge number of variations of GANs [see <a href="https://github.com/hindupuravinash/the-gan-zoo">https://github.com/hindupuravinash/the-gan-zoo</a> for list]. This overview only deals with some of the more prominent recent developments and the most commonly mentioned in particular fields. There are also a number of reviews of the field, for example see <a href="https://ieeexplore.ieee.org/document/8195348">https://ieeexplore.ieee.org/document/8195348</a>. For strictly generative models, see <a href="https://blog.openai.com/generative-models/">https://blog.openai.com/generative-models/</a>; for neural networks see <a href="http://www.asimovinstitute.org/neural-network-zoo/">http://www.asimovinstitute.org/neural-network-zoo/</a>; for an overview of the field of deep learning see <a href="https://www.nature.com/articles/nature14539">https://www.nature.com/articles/nature14539</a>.
</p>
</div>
</div>
<div id="outline-container-sec-1-4" class="outline-3">
<h3 id="sec-1-4"><span class="section-number-3">1.4</span> Architectures</h3>
<div class="outline-text-3" id="text-1-4">
</div><div id="outline-container-sec-1-4-1" class="outline-4">
<h4 id="sec-1-4-1"><span class="section-number-4">1.4.1</span> General Models</h4>
<div class="outline-text-4" id="text-1-4-1">
</div><ol class="org-ol"><li><a id="sec-1-4-1-1" name="sec-1-4-1-1"></a>Feed-forward Neural Network (FFNN or NN) (aka vanilla, or just 'neural network')<br  /></li>
<li><a id="sec-1-4-1-2" name="sec-1-4-1-2"></a>Deep Neural Network (DNN)<br  /></li></ol>
</div>
<div id="outline-container-sec-1-4-2" class="outline-4">
<h4 id="sec-1-4-2"><span class="section-number-4">1.4.2</span> Discrimination and Classification</h4>
<div class="outline-text-4" id="text-1-4-2">
<p>
Convolutional neural networks are typically used for image classification problems, as they have had much more success recently in terms of accuracy, particularly in the case of using Deep CNNs with large datasets. Convolutional neural networks use multiple layers of convolutional processes that activate based on features in the data (such as edges or other identifying features). These are then reduced in dimensionality using max pooling and stride convolutional layers. These networks also typically use Rectified Linear Units (ReLU) to speed up training (convergence) and introduce non-linearity. The layers are then combined using fully connected layers and a classification is determined using softmax or other functions to calculate a final probability.
</p>

<p>
This may be of importance to this research if we perform some kind of classification, or if training semi-supervised GANs. More detail on specific implementations of convolutional neural networks below.
</p>
</div>
<ol class="org-ol"><li><a id="sec-1-4-2-1" name="sec-1-4-2-1"></a>Convolutional Neural Network (CNN)<br  /></li>

<li><a id="sec-1-4-2-2" name="sec-1-4-2-2"></a>De-convolutional Network (DN)<br  /></li></ol>
</div>
<div id="outline-container-sec-1-4-3" class="outline-4">
<h4 id="sec-1-4-3"><span class="section-number-4">1.4.3</span> Memory Networks</h4>
<div class="outline-text-4" id="text-1-4-3">
<p>
These architectures use memory cells to duplicate the effect of a neural network across a sequence (of time, pixels, audio samples etc.). They are effective for generating sequences of text or sound, and have been used to generate images through implementations such as PixelRNN. The images they generate are typically not as realistic as GANs. Recurrent networks can also have difficulties with exploding/vanishing weights while training. PixelRNN is relatively efficient to train but inference (generating images) can be slow and resource-intensive.
</p>
</div>
<ol class="org-ol"><li><a id="sec-1-4-3-1" name="sec-1-4-3-1"></a>Recurrent Neural Network (RNN)<br  /><div class="outline-text-5" id="text-1-4-3-1">
<p>
<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a>
</p>
</div>
</li>
<li><a id="sec-1-4-3-2" name="sec-1-4-3-2"></a>Long Short-Term Memory (LSTM)<br  /><div class="outline-text-5" id="text-1-4-3-2">
<p>
This is a kind of recurrent neural network that uses a particular type of memory cell. These cells have three different gate mechanisms for input, output, and memory.
<a href="https://medium.com/datathings/the-magic-of-lstm-neural-networks-6775e8b540cd">https://medium.com/datathings/the-magic-of-lstm-neural-networks-6775e8b540cd</a>
</p>
</div>
</li>
<li><a id="sec-1-4-3-3" name="sec-1-4-3-3"></a>Neural Turing Machine<br  /><div class="outline-text-5" id="text-1-4-3-3">
<p>
A type of neural network that has access to external memory. Can solve some complex problems, somewhat similar to LSTM.
<a href="https://arxiv.org/pdf/1410.5401.pdf">https://arxiv.org/pdf/1410.5401.pdf</a>
</p>
</div>
</li></ol>
</div>
<div id="outline-container-sec-1-4-4" class="outline-4">
<h4 id="sec-1-4-4"><span class="section-number-4">1.4.4</span> Image Generation</h4>
<div class="outline-text-4" id="text-1-4-4">
<p>
Generative networks are generally unsupervised, although some use classifications or images to condition the generated images. Generative networks generally attempt to estimate the density of the data distribution. Different approaches to image generation attempt to create an encoder that can turn a 'code' or set of 'latent variables' into an image that closely resembles the patterns and features of the training data. GANs are the most successful in creating realistic images but can be difficult to train as they require both the generator and discriminator to be working well together. VAEs and Flow-based models are easier to train but generally do not output images that are as realistic. PixelRNN and PixelCNN work relatively efficiently but are computationally expensive.
</p>
</div>
<ol class="org-ol"><li><a id="sec-1-4-4-1" name="sec-1-4-4-1"></a>Generative Adversarial Network (GAN)<br  /><div class="outline-text-5" id="text-1-4-4-1">
<p>
GANs use two models, a generator and a discriminator, which work together to produce high quality images. The generator attempts to create images that closely resemble the training or 'real' data. The discriminator attempts to determine if the image is generated or from the training set. Both networks need to be trained together and must simultaneously improve in accuracy in order to produce images that more closely resemble the training data. The two models are playing a 'minimax game'.
</p>

<p>
<a href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a>
Recent tutorial: <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29">https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29</a>
</p>
</div>
</li>
<li><a id="sec-1-4-4-2" name="sec-1-4-4-2"></a>Variational Auto Encoder (VAE)<br  /><div class="outline-text-5" id="text-1-4-4-2">
<p>
First part of the network encodes an input to a set of latent features, the second part of the network decodes the latent features to an output that closely resembles the input. The latent features are a bottleneck, i.e. use less data to represent a given input/output. Can therefore be used for compression, but can also be used for image generation etc.
</p>

<p>
Different variations of auto-encoders are able to capture particular individual characteristics within latent features so they can manipulated or interpolated separately, e.g. lighting, angle, position. Generally produces images that are somewhat blurry or less clear than GANs, but are somewhat more stable and therefore easier to train.
</p>

<p>
<a href="http://kvfrans.com/variational-autoencoders-explained/">http://kvfrans.com/variational-autoencoders-explained/</a>
</p>
</div>
</li>
<li><a id="sec-1-4-4-3" name="sec-1-4-4-3"></a>PixelRNN and PixelCNN - WaveNet<br  /><div class="outline-text-5" id="text-1-4-4-3">
<p>
Useful for sound and image generation. For images, these networks work pixel by pixel: each pixel is calculated based on the pixel(s) previous (using p1 as input for p2, p1+p2 as input for p3, etc.). They may also additionally work using pixels from the row above or in a circular area around the current pixel. This gives a good representation of the data distribution but can be somewhat slow in both training and inference.
</p>

<p>
<a href="https://arxiv.org/abs/1601.06759">https://arxiv.org/abs/1601.06759</a>
<a href="https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173">https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173</a>
<a href="https://towardsdatascience.com/summary-of-pixelrnn-by-google-deepmind-7-min-read-938d9871d6d9">https://towardsdatascience.com/summary-of-pixelrnn-by-google-deepmind-7-min-read-938d9871d6d9</a>
</p>
</div>
</li>
<li><a id="sec-1-4-4-4" name="sec-1-4-4-4"></a>Conditional Generative Adversarial Network (cGAN) - pix2pix<br  /><div class="outline-text-5" id="text-1-4-4-4">
<p>
Uses a condition (or set of conditions) on the generator in order to generate a matching output. Implemented recently as pix2pix for image-to-image translation, see <a href="https://phillipi.github.io/pix2pix/">https://phillipi.github.io/pix2pix/</a>. pix2pix creates a general purpose model that can be trained to perform well on a wide variety of image to image tasks. Examples of success include translating from a drawing of edges to a colour image, or from segmentation markers to near-photographic images. Uses composite loss of a GAN and a regression term.
</p>

<p>
<a href="https://tcwang0509.github.io/pix2pixHD/">https://tcwang0509.github.io/pix2pixHD/</a>
Web demo: <a href="https://affinelayer.com/pixsrv/">https://affinelayer.com/pixsrv/</a>
</p>
</div>
</li>
<li><a id="sec-1-4-4-5" name="sec-1-4-4-5"></a>Cascaded Refinement Networks (CRN)<br  /><div class="outline-text-5" id="text-1-4-4-5">
<p>
A supervised convolutional network that uses refinement layers. It is conditioned using an input image of segments (such as blobs for cars/trees in a road scene, or furniture/walls in an interior scene) which it uses to produce a pseudo-photorealistic image. Does not use adversarial networks, trained using a direct regression objective.
</p>

<p>
<a href="https://arxiv.org/abs/1707.09405">https://arxiv.org/abs/1707.09405</a>
<a href="https://github.com/CQFIO/PhotographicImageSynthesis">https://github.com/CQFIO/PhotographicImageSynthesis</a>
</p>
</div>
</li>
<li><a id="sec-1-4-4-6" name="sec-1-4-4-6"></a>Generative Latent Nearest Neighbours (GLANN)<br  /><div class="outline-text-5" id="text-1-4-4-6">
<p>
Another non-GAN approach to image generation. Uses aspects of latent embedding learning methods (e.g. GLO) and nearest-neighbour based implicit maximum likelihood estimation (IMLE). Produces good results and does not suffer from some of the problems with training GANs. Can also perform image translation by interpolating across input noise.
</p>

<p>
Published Dec 2018. <a href="https://arxiv.org/abs/1812.08985v1">https://arxiv.org/abs/1812.08985v1</a>
</p>
</div>
</li>
<li><a id="sec-1-4-4-7" name="sec-1-4-4-7"></a>Semi-parametric Image Synthesis (SIMS)<br  /><div class="outline-text-5" id="text-1-4-4-7">
<p>
Another approach to image synthesis from an input conditioning image. Most effective at translating from a segmentation image to a pseudo-photorealistic image, with some parameters for variation.
</p>

<p>
<a href="https://arxiv.org/abs/1804.10992">https://arxiv.org/abs/1804.10992</a>
<a href="https://www.youtube.com/watch?v=U4Q98lenGLQ">https://www.youtube.com/watch?v=U4Q98lenGLQ</a>
<a href="https://www.reddit.com/r/MachineLearning/comments/8g9k0s/r_photographic_image_generation_with/">https://www.reddit.com/r/MachineLearning/comments/8g9k0s/r_photographic_image_generation_with/</a>
</p>
</div>
</li>
<li><a id="sec-1-4-4-8" name="sec-1-4-4-8"></a>Flow-based Deep Generative Models (Flow)<br  /><div class="outline-text-5" id="text-1-4-4-8">
<p>
Architecture that explicitly learns the probability distribution of the data p(x) through invertible functions. This means that it is straightforward to train and maps well to the probabilities of the input data. Also avoids using GAN techniques. Main example appears to be GLOW produced by OpenAI. Images produced are not as clear or crisp as state-of-the-art GANs, and manipulating specific features seems to produce many artifacts, however interpolation of z vectors seems to work well (based on examples in OpenAI blog post).
</p>

<p>
<a href="https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html#glow">https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html#glow</a>
<a href="https://blog.openai.com/glow/">https://blog.openai.com/glow/</a>
<a href="https://github.com/openai/glow">https://github.com/openai/glow</a>
</p>
</div>
</li></ol>
</div>
</div>
<div id="outline-container-sec-1-5" class="outline-3">
<h3 id="sec-1-5"><span class="section-number-3">1.5</span> Implementations</h3>
<div class="outline-text-3" id="text-1-5">
<p>
Some of the above describe both the architecture and the specific implementation. Below some of the more specific implementations of networks are covered.
</p>
</div>
<div id="outline-container-sec-1-5-1" class="outline-4">
<h4 id="sec-1-5-1"><span class="section-number-4">1.5.1</span> Deep Convolutional Neural Networks (classification)</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
These are variations of Deep Convolutional Neural Networks. They vary in size and complexity. Some introduce new features or re-organise layers and neurons in particular ways. The following have largely been tested on the ImageNet dataset and have made incremental improvements in both top-1 and top-5 accuracy for classification.
</p>

<p>
AlexNet was the first famous CNN.
VGG used 3x3 convolutional filters and placed max pooling layers after each 2 convolutions, doubling the number of filters after each max pooling. VGG is much deeper than AlexNet.
GoogLeNet uses inception modules that use pooling, convolution, and concatenation at different scales.
ResNet uses residual layers, incorporating memory into CNNs. This allows for additional depth while still training effectively..
</p>

<p>
Pre-trained models for each of these are available at: <a href="https://keras.io/applications/#vgg19">https://keras.io/applications/#vgg19</a>
</p>
</div>
<ol class="org-ol"><li><a id="sec-1-5-1-1" name="sec-1-5-1-1"></a>AlexNet<br  /></li>
<li><a id="sec-1-5-1-2" name="sec-1-5-1-2"></a>VGG16 and VGG19<br  /></li>
<li><a id="sec-1-5-1-3" name="sec-1-5-1-3"></a>ResNet50<br  /></li>
<li><a id="sec-1-5-1-4" name="sec-1-5-1-4"></a>Inception Architectures<br  /><div class="outline-text-5" id="text-1-5-1-4">
<p>
These implementations use "inception modules" which are parallel convolutional processes that can be learned as weights. These allow the model to learn whether it is better to use a 3x3 or 5x5 convolution, for example. These implementations also use 1x1 convolutions as a means towards feature reduction, by having a lower number of filters than the number of features in the input.
</p>
</div>
<ol class="org-ol"><li><a id="sec-1-5-1-4-1" name="sec-1-5-1-4-1"></a>Inception v1-3 (GoogLeNet)<br  /></li>
<li><a id="sec-1-5-1-4-2" name="sec-1-5-1-4-2"></a>InceptionResNetV2<br  /></li></ol>
</li>
<li><a id="sec-1-5-1-5" name="sec-1-5-1-5"></a>Xception<br  /></li>
<li><a id="sec-1-5-1-6" name="sec-1-5-1-6"></a>MobileNet<br  /></li>
<li><a id="sec-1-5-1-7" name="sec-1-5-1-7"></a>MobileNetV2<br  /></li>
<li><a id="sec-1-5-1-8" name="sec-1-5-1-8"></a>DenseNet<br  /></li>
<li><a id="sec-1-5-1-9" name="sec-1-5-1-9"></a>NASNet<br  /></li></ol>
</div>
<div id="outline-container-sec-1-5-2" class="outline-4">
<h4 id="sec-1-5-2"><span class="section-number-4">1.5.2</span> Generative Adversarial Networks</h4>
<div class="outline-text-4" id="text-1-5-2">
</div><ol class="org-ol"><li><a id="sec-1-5-2-1" name="sec-1-5-2-1"></a>Style GAN<br  /><div class="outline-text-5" id="text-1-5-2-1">
<p>
Latest development from the NVidia research group (Karras, Laine, Aila). A ProgGAN that additionally learns to separate different aspects of the images without supervision. Trained on the Flickr-Faces-HQ dataset (FFHQ) and used to produce images of faces (also demonstrated on cars, bedrooms, and cats). Effectively treats each face as the combination of a number of styles that can be divided into coarse, middle, and fine, each altering the produced image in different ways. These can be combined in different ways so that the latent space can be interpolated and navigated in ways that demonstrate the way that particular styles are combined.
</p>

<p>
Builds on ProgGAN but attempts to combine this with techniques of style transfer.
</p>

<p>
<a href="https://www.youtube.com/watch?time_continue=3&v=kSLJriaOumA">https://www.youtube.com/watch?time_continue=3&v=kSLJriaOumA</a>
<a href="https://arxiv.org/abs/1812.04948">https://arxiv.org/abs/1812.04948</a>
</p>
</div>
</li>
<li><a id="sec-1-5-2-2" name="sec-1-5-2-2"></a>Progressive GAN (PG GAN or ProgGAN)<br  /><div class="outline-text-5" id="text-1-5-2-2">
<p>
Progressive training of GAN used by NVidia to generate high resolution, pseudo-photorealistic images of celebrity faces. Also demonstrated on LSUN and CIFAR categories. Grows both the generator and discriminator progressively. Starts from a low resolution and then adding new layers the model increasingly fine details as training progresses. This helps to speed up and stabilise training, as well as producing high quality, high resolution images.
<a href="https://github.com/tkarras/progressive_growing_of_gans">https://github.com/tkarras/progressive_growing_of_gans</a> (tensorflow)
<a href="https://arxiv.org/abs/1710.10196">https://arxiv.org/abs/1710.10196</a>
</p>
</div>
</li>
<li><a id="sec-1-5-2-3" name="sec-1-5-2-3"></a>BigGAN<br  /><div class="outline-text-5" id="text-1-5-2-3">
<p>
A very large-scale GAN created by Google. Trained at largest scale attempted so far, required between 128-512 TPUs for different image sizes. Uses a number of modifications on typical GANs to produce improvements in regularisation and image quality. Uses class conditions to generate images of a particular type (e.g. dog). Uses huge amounts of computing power.
Some artists and coders have experimented with moving through the latent space [<a href="https://thegradient.pub/bigganex-a-dive-into-the-latent-space-of-biggan/">https://thegradient.pub/bigganex-a-dive-into-the-latent-space-of-biggan/</a>].
</p>

<p>
The code implementation is available online, but only through Google's colab service. This offers a GPU system for free for research purposes. Model cannot be downloaded or trained, but can output images according to different z vectors (changing position in the latent space).
<a href="https://arxiv.org/pdf/1809.11096.pdf">https://arxiv.org/pdf/1809.11096.pdf</a>
<a href="https://tfhub.dev/s?q=biggan">https://tfhub.dev/s?q=biggan</a>
<a href="https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb">https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb</a>
</p>
</div>
</li>
<li><a id="sec-1-5-2-4" name="sec-1-5-2-4"></a>Self Attention GAN (SAGAN)<br  /><div class="outline-text-5" id="text-1-5-2-4">
<p>
A convolutional GAN that introduces a self-attention mechanism. Helps to model dependencies across image regions. Also applies spectral normalisation to the discriminator network, which helps to stabilise training. Creates good images that are crisp and seemingly have good backgrounds and coexisting features. Can produce images according to categories. Has also been used for text to image synthesis.
</p>

<p>
<a href="https://arxiv.org/abs/1805.08318">https://arxiv.org/abs/1805.08318</a>
Code implementation available: <a href="https://github.com/brain-research/self-attention-gan">https://github.com/brain-research/self-attention-gan</a>
Tutorial: <a href="https://medium.com/@jonathan_hui/gan-self-attention-generative-adversarial-networks-sagan-923fccde790c">https://medium.com/@jonathan_hui/gan-self-attention-generative-adversarial-networks-sagan-923fccde790c</a>
</p>
</div>
</li>
<li><a id="sec-1-5-2-5" name="sec-1-5-2-5"></a>CycleGAN<br  /><div class="outline-text-5" id="text-1-5-2-5">
<p>
Image-to-image translation where the paired data is not available. Able to translate an image from a source domain X to a target domain Y. Similar to style transfer in some regards, able to translate a photographic image to "Monet" (or the reverse), horse to zebra, winter to summer etc. Also able to work from segmentation images to pseudo-photorealistic images (similar to pix2pix et al).
</p>

<p>
<a href="https://arxiv.org/abs/1703.10593">https://arxiv.org/abs/1703.10593</a>
<a href="https://junyanz.github.io/CycleGAN/">https://junyanz.github.io/CycleGAN/</a>
</p>
</div>
</li>
<li><a id="sec-1-5-2-6" name="sec-1-5-2-6"></a>Stack GAN<br  /><div class="outline-text-5" id="text-1-5-2-6">
<p>
GAN that generates high-quality images from text description conditions. Uses multiple stages of GANs that firstly generate primitive shapes and colours, then generates high-resolution images. Creates fairly good quality images, but is sometimes described as "slow".
</p>

<p>
<a href="https://arxiv.org/abs/1612.03242">https://arxiv.org/abs/1612.03242</a>
<a href="https://www.reddit.com/r/MachineLearning/comments/9p3jdz/r_tdls_stackgan_realistic_image_synthesis_with/">https://www.reddit.com/r/MachineLearning/comments/9p3jdz/r_tdls_stackgan_realistic_image_synthesis_with/</a>
<a href="https://arxiv.org/abs/1710.10916">https://arxiv.org/abs/1710.10916</a>
</p>
</div>
</li>
<li><a id="sec-1-5-2-7" name="sec-1-5-2-7"></a>Info GAN<br  /><div class="outline-text-5" id="text-1-5-2-7">
<p>
A GAN that learns latent variables without labels. Attempts to disentangle representations without supervision, e.g. learn latent variables in MNIST such as stroke thickness. This allows individual features such as lighting, pose, or rotation to be changed separately.
</p>

<p>
Published Jun 2016: <a href="https://arxiv.org/abs/1606.03657">https://arxiv.org/abs/1606.03657</a>
</p>
</div>
</li>
<li><a id="sec-1-5-2-8" name="sec-1-5-2-8"></a>Wasserstein GAN<br  /><div class="outline-text-5" id="text-1-5-2-8">
<p>
A modification of GANs that attempts to alleviate some of the issues with training.
<a href="https://arxiv.org/abs/1701.07875">https://arxiv.org/abs/1701.07875</a>
</p>
</div>
</li>
<li><a id="sec-1-5-2-9" name="sec-1-5-2-9"></a>Deep Convolutional Generative Adversarial Network (DCGAN)<br  /><div class="outline-text-5" id="text-1-5-2-9">
<p>
Paper by Radford from early 2016. Most GANs now use convolutional layers. Proven to be more effective than vanilla neural networks, particularly for image generation.
<a href="https://github.com/carpedm20/DCGAN-tensorflow">https://github.com/carpedm20/DCGAN-tensorflow</a>
<a href="https://arxiv.org/pdf/1511.06434v2.pdf">https://arxiv.org/pdf/1511.06434v2.pdf</a> (original paper)
</p>

<p>
Pytorch tutorial: <a href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html">https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html</a>
</p>
</div>
</li></ol>
</div>
<div id="outline-container-sec-1-5-3" class="outline-4">
<h4 id="sec-1-5-3"><span class="section-number-4">1.5.3</span> Variational Auto-Encoders (VAE)</h4>
<div class="outline-text-4" id="text-1-5-3">
</div><ol class="org-ol"><li><a id="sec-1-5-3-1" name="sec-1-5-3-1"></a>Deep Recurrent Attentive Writer (DRAW)<br  /><div class="outline-text-5" id="text-1-5-3-1">
<p>
Uses a spatial mechanism with a sequential VAE framework that allows it to iteratively construct complex images. Published early/mid 2015.
<a href="https://arxiv.org/abs/1502.04623">https://arxiv.org/abs/1502.04623</a>
</p>
</div>
</li>
<li><a id="sec-1-5-3-2" name="sec-1-5-3-2"></a>Attend Infer Repeat (AIR)<br  /><div class="outline-text-5" id="text-1-5-3-2">
<p>
"AIR aims to reconstruct an image, but instead of doing it in a single shot, it focuses on interesting image parts one-by-one. The figure below demonstrates AIR’s inner workings. It takes a look at the image, figures out how many interesting parts there are and where they are in the image. It then reconstructs them by painting one-part-at-a-time onto a blank canvas. AIR takes a look at the image, figures out how many interesting parts there are, and reconstructs it by painting one-part-at-a-time onto a blank canvas." &#x2013; Adam Kosiorek [<a href="http://akosiorek.github.io/ml/2017/09/03/implementing-air.html">http://akosiorek.github.io/ml/2017/09/03/implementing-air.html</a>]
</p>

<p>
Seems to be an interesting but complicated and somewhat fragile architecture. Only provides examples using multi-MNIST data.
</p>

<p>
August 2016
<a href="https://arxiv.org/abs/1603.08575">https://arxiv.org/abs/1603.08575</a>
Tutorial for implementation: <a href="http://akosiorek.github.io/ml/2017/09/03/implementing-air.html">http://akosiorek.github.io/ml/2017/09/03/implementing-air.html</a>
Update/improvement: <a href="https://arxiv.org/abs/1806.01794">https://arxiv.org/abs/1806.01794</a>
Code: <a href="http://pyro.ai/examples/air.html">http://pyro.ai/examples/air.html</a>
</p>
</div>
</li>
<li><a id="sec-1-5-3-3" name="sec-1-5-3-3"></a>DCIGN (Deep Convolutional Inverse Graphics Network)<br  /><div class="outline-text-5" id="text-1-5-3-3">
<p>
This network is a VAE but with CNNs and DNs as encoder and decoder.
<a href="https://arxiv.org/pdf/1503.03167v4.pdf">https://arxiv.org/pdf/1503.03167v4.pdf</a>
</p>
</div>
</li></ol>
</div>
</div>
<div id="outline-container-sec-1-6" class="outline-3">
<h3 id="sec-1-6"><span class="section-number-3">1.6</span> Conclusion</h3>
<div class="outline-text-3" id="text-1-6">
<p>
The current state of the art for generating images appears to utilise GANs with various modifications, however there are some other architectures that also do well at image generation. For our dataset size (several million images, potentially divided into different classifications according to research area) perhaps the best approach would be to use a GAN with supervised training, incorporating features from the Progressive GAN and Self Attention GAN in order to improve training stability as well as image quality. The potential problems of using GANs include difficulties in training and instability, as well as requiring large amounts of computing power. Perhaps the best strategy then is to use pre-trained models that can then be trained on our dataset to see if this produces results quickly, as well as testing some existing code implementations of these architectures using a subset of our data. This should provide some indication of which architectures are feasible and likely to produce good results for this particular task.
</p>

<p>
Many of these models will be able to generate relatively high quality images based on our dataset, as well as be able to learn features of the dataset that could be manipulated and navigated by interpolating between vectors in latent space.
</p>

<p>
The best performing architecture for text generation appears to still be recurrent neural networks and in particular those using Long Short Term Memory (LSTM). Given the size of our dataset, it is feasible to divide into research area (e.g. mathematics) and then train using this corpus. May produce some interesting results.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Re-imaging</p>
<p class="date">Created: 2019-01-23 Wed 17:00</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.2.2 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
