#+OPTIONS: toc:nil
#+OPTIONS: ^:nil
#+OPTIONS: *:nil
* Re-imaging the empirical - Documentation
** Contents :TOC:QUOTE:noexport:
#+BEGIN_QUOTE
- [[#re-imaging-the-empirical---documentation][Re-imaging the empirical - Documentation]]
  - [[#about][About]]
  - [[#deepcluster][DeepCluster]]
  - [[#ternary-classifier][Ternary Classifier]]
  - [[#feature-visualisation][Feature Visualisation]]
  - [[#t-sne][t-SNE]]
  - [[#umap][UMAP]]
  - [[#stylegan][StyleGAN]]
  - [[#msg-gan][MSG-GAN]]
#+END_QUOTE

** About
This page describes and gives examples of some of the processes undertaken in the project Re-imaging the empirical: statistical visualisation in art and science. For more info about the project see https://github.com/re-imaging/re-imaging
** DeepCluster
We used the method of unsupervised learning outlined in the arXiv paper [[https://arxiv.org/abs/1807.05520][Deep Clustering for Unsupervised Learning of Visual Features]] to train a classifier that predicts whether images belong to 100 different clusters. This produces strong similarity among particular image trends, e.g. logos, schematics, photographs, graphs showing small multiples. This provides the highest degree of learning based on the arXiv images dataset. For more details on the code used, see the [[https://github.com/facebookresearch/deepcluster][DeepCluster GitHub]].

We trained the model on 100,000 randomly selected images from the arXiv dataset, using 100 clusters (learned during training). This was trained for just over 1000 epochs. The clustering produced by this model shows a high likelyhood to group what humans might perceive are similar image types together, e.g. text or logos, photographs of people or richly coloured images, graphs that have been rotated 90 degrees.

You can view montages showing 256 images for all 100 clusters here: https://www.dropbox.com/sh/2a16em11h5kiw5c/AADILqH17EaPWLxeaxIqCNpka?dl=0

Examples:

#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/DeepCluster/36_montage.jpg" /></p>
---
#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/DeepCluster/2_montage.jpg" /></p>
---
#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/DeepCluster/3_montage.jpg" /></p>
---
#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/DeepCluster/3_montage.jpg" /></p>
---

** Ternary Classifier
"To estimate the ratios of these different types of images, we trained a neural network classifier that would predict whether an image most closely resembled a ‘diagram,’ ‘sensor’ or ‘mixed’ (this work was performed by the three researchers on this project and assigned independently to each as a human intelligence task or HIT. Our decisions are of necessity biased according to our individual and collective approaches to labelling these categories). To build a training dataset, we manually labelled 9748 randomly sampled images. The decision to create a ternary classifier with a ‘mixed’ category was intended to capture and highlight the varied operations being performed by images across different kinds of knowledge domains. For example, in our initial querying of image distribution in the dataset, we noticed clusters of images that appeared to be somehow sensor-generated yet re-organised through graphic overlays that seemed to suggest they were being set within a diagrammatic schema. For our purposes, then, a ternary labelling and classification of the images allowed us to view the shifting ratios of image types across arXiv. Our training set produced: 8649 (88.7%) images as ‘diagram,’ 477 (4.89%) as ‘sensor’ and 622 (6.38%) images as ‘mixed.’"

More documentation:
- prediction by image: https://www.dropbox.com/sh/pxt2nriu72zcgfw/AADCrmnreDFJZRP3KpIXusV4a?dl=0
- similarity strips: https://www.dropbox.com/sh/x4cyrn29yv32nde/AAAHW15aNwlhpPQYV2B6RV_qa?dl=0
- prediction confusion matrix: https://www.dropbox.com/sh/av7qj2zlluyaz0j/AABjJy0QYsBft0UKOAgAqAK5a?dl=0

Examples of prediction confidence on a number of test images:

#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/ternary/ternary_prediction_diagram_68pc_5229196.jpg" /></p
>
#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/ternary/ternary_prediction_diagram_97pc_3845003.jpg" /></p>

#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/ternary/ternary_prediction_mixed_99pc_1018846.jpg" /></p>

#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/ternary/ternary_prediction_sensor_99pc_5408067.jpg" /></p>

** Feature Visualisation
Using [[https://github.com/zentralwerkstatt/explain.ipynb][explain.ipynb]] by Fabian Offert, we used gradient ascent to visualise the features of the Ternary Classifier. This creates an image that most strongly activates the given prediction class.

More examples: https://www.dropbox.com/sh/qcv424p32xjmwqf/AADi_Hp20uW7BF8OojVOxKoOa?dl=0

#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/feature-visualisation/diagram_vis_x2500_e500.jpg" /></p>
#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/feature-visualisation/sensor_vis_x2500_e500.jpg" /></p>
#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/feature-visualisation/unsure_vis_x2500_e500.jpg" /></p>

https://www.dropbox.com/sh/p27ifjfdcpfdx28/AAATXzIwB-1lDEZ0tarf8tnba?dl=0
** t-SNE
** UMAP
** StyleGAN
StyleGAN: https://github.com/NVlabs/stylegan
running with TensorFlow
trained for ~3 days on GTX 2080
training dataset: 10k or 100k random images from arXiv dataset
images downsampled and cropped to 256x256
resolution limited to 256x256

videos:
- training process
- random interpolations
- finer-grain interpolation (latent space changes affecting finer-grain details)
- circular interpolation (moving through latent space in a way that approximates a sphere and minimises changes in mean square error)

images:
- fakes*****.png: these show the same points in latent space (z) over the course of training
- truncation trick values (ex-1.2, ex-0.7, ex-0.0): truncates the range of values in the randomly generated latent variable z. Truncation of 0.0 means there is very little variation and approximates a "mean" or "average" image for the learned distribution. Truncation of 1.2 means high levels of variation. Truncation is usually limited to ~0.7 in order to produce "better" images that are more likely to be part of the distribution and reduce large spikes in the z variables
- examples: shows different examples from the original Karras et al paper but using the current trained model

Video of training process: https://www.dropbox.com/s/k97p1rr2p9xloxr/stylegan-test-training-fullsize.mp4?dl=0
  
Documentation of 100k image dataset trained for 233 epochs: https://www.dropbox.com/sh/u4g4llofgg3mz1l/AABBAh9re-0IHejytDJXoszMa?dl=0

Examples montages at varying values of truncation:

#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/stylegan/montage512_example-0.0.jpeg" /></p>
0.0
#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/stylegan/montage512_example-0.7.jpeg" /></p>
0.7
#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/stylegan/montage512_example-1.5.jpeg" /></p>
1.5
** MSG-GAN
Using code from [[https://github.com/akanimax/msg-gan-v1][Multi-Scale Gradients GAN]]. Trained for 100 epochs on a small subset of arXiv images.

Video of training process: https://www.dropbox.com/s/xjk71q80yt1bsj0/msggan_training_256x256x36_100epochs_vid.mp4?dl=0

#+html: <p align="center"><img src="https://github.com/re-imaging/re-imaging/blob/master/figures/MSG-GAN/gen_0099_1632.png" /></p>
