
* Setting Up SQLite Database
Instructions for creating and populating the database for arXiv images.

** Installing software
*** Metha
https://github.com/miku/metha

*** SQLite (command line)
Ubuntu ships with SQLite. Simply call

$ sqlite3

*** Python SQLite
This is included in Python, just import into scripts:

import sqlite

*** DBBrowser for SQLite (optional)
https://sqlitebrowser.org/dl/

sudo add-apt-repository -y ppa:linuxgndu/sqlitebrowser
sudo apt-get update
sudo apt-get install sqlitebrowser

** Downloading Metha OAI XML files

arXiv participates in the Open Archives Initiative and provides up-to-date metadata for the full archive. Full bulk metadata is therefore downloadable using the OAI-PMH v2.0 protocol (Lagoze et. al., 2001). These records are stored in XML format and contain details such as the identifier, author, title, categories, licence, abstract, and date. The format specification of the metadata used for this project is arXiv, although other formats are available that provide slightly different data fields. We downloaded the metadata for all articles (records) up to the end of 2018 using the Metha OAI harvesting tool. After installing Metha, usage is as simple as calling:

#+BEGIN_SRC bash
metha-sync -format arXiv http://export.arxiv.org/oai2 
#+END_SRC

which will download all OAI records in gzipped XML format (Extensible Markup Language) using the arXiv format. In our case, this resulted in 1575 separate gzipped XML files that could then be parsed through to retrieve metadata fields.
** Extract Metha gz files

#+BEGIN_SRC bash
# back up files first, this converts in-place
# change to directory with metha-downloaded oai gz files

for f in *.gz; do gunzip "$f"; done
#+END_SRC

** Example metadata
<ListRecords>
    <record>
        <header status="">
            <identifier>oai:arXiv.org:0704.0004</identifier>
            <datestamp>2007-05-23</datestamp>
            <setSpec>math</setSpec>
        </header>
        <metadata>
            <arXiv xsi:schemaLocation="http://arxiv.org/OAI/arXiv/                             http://arxiv.org/OAI/arXiv.xsd">
                <id>0704.0004</id>
                <created>2007-03-30</created>
                <authors>
                    <author>
                        <keyname>Callan</keyname>
                        <forenames>David</forenames>
                    </author>
                </authors>
                <title>A determinant of Stirling cycle numbers counts unlabeled acyclic single-source automata</title>
                <categories>math.CO</categories>
                <comments>11 pages</comments>
                <msc-class>05A15</msc-class>
                <abstract>  We show that a determinant of Stirling cycle numbers counts unlabeled acyclic single-source automata. The proof involves a bijection from these automata to certain marked lattice paths and a sign-reversing involution to evaluate the determinant.
                </abstract>
            </arXiv>
        </metadata>
    <about/>
</record>

Figure XX. Example XML file in the “arXiv” format showing organisation of metadata according to specific tags such as “categories”, “title”, “author”, “abstract” etc. Example shown here is Callan, David: A determinant of Stirling cycle numbers counts unlabeled acyclic single-source automata, 2007, https://arxiv.org/abs/0704.0004

** Create SQLite database
Python script to create SQLite database

Usage:

#+BEGIN_SRC bash
python create-db/sqlite_create_db.py
#+END_SRC

Details or table creation in the script:

#+BEGIN_SRC Python
# create metadata table
c.execute('''
    CREATE TABLE metadata(id INTEGER PRIMARY KEY, identifier TEXT, created TEXT, \
    cat TEXT, authors TEXT, title TEXT, abstract TEXT, licence TEXT)
''')

# create images table
c.execute('''
    CREATE TABLE images (id INTEGER PRIMARY KEY, identifier TEXT, filename TEXT, \
    filesize INT, path TEXT, x INT, y INT, imageformat TEXT)
''')
#+END_SRC
** Inserting article metadata into database

We used a set of Python scripts to then iterate over the XML files and insert rows into the database for each. For this step we used the Python ElementTree XML API to navigate the required XML files and extract the desired metadata. This required custom code that was tailored to the specific XML format as it is stored on arxiv.org and downloaded by metha (which groups XML documents together to reduce the number of files).

In order to index the images with their associated article metadata, we created a SQLite database with two tables: metadata with rows for each article; and images with rows for each image file. SQLite was chosen as a database format because of its widespread use and the simplicity of a lightweight, single-file database in contrast to client-server database formats. SQLite performs well within the expected number of entries (tables/columns/rows). SQLite is also a reasonably common format and has command line, R, and Python interfaces, and can also be read by other common data science tools such as the Python Data Analysis Library, a.k.a. pandas.

For our purposes, we used a primary key of a unique number, and inserted the identifier, date created, categories, authors, title, abstract, and licence. 

The oai_to_sqlite.py script accesses a folder of Metha-downloaded OAI XML files.

#+BEGIN_SRC bash
usage: oai_to_sqlite.py [-h] [-v] db_path oai_path

Parse Matha OAI XML files and insert metadata into SQLite database

positional arguments:
  db_path        path to SQLite database
  oai_path       set folder of OAI xml files

optional arguments:
  -h, --help     show this help message and exit
  -v, --verbose  verbose output
#+END_SRC

Example usage
#+BEGIN_SRC bash
python oai_to_sqlite.py ~/data/db/arxiv_db.sqlite ~/data/oai/metha/
#+END_SRC
** Image metadata

In order to add rows of image data to the database, we searched and iterated over all of the directory structure of the arXiv bulk downloads and gathered metadata directly from each file. We found that an effective way to do this was to use the unix ```find``` command to write all image file paths to a text file, then using those paths to run an identify command (from Imagemagick) to write a number of details to the database. We collected the article identifier, filename, filesize, filepath, x dimension size, y dimension size, and imageformat. This took a number of days to complete, but could be optimised using faster storage media and/or parallel processing.

This is done as a two step process because it will take a long time and it is helpful to be able to restart the process partway. (If you'd like to do it in one step, examples are in create-db/additional.)

#+BEGIN_SRC bash
# first we need to get the paths of all the image files
cd create-db

# this will take a little while
./image_paths_to_txt.sh SOURCE_DIR TARGET_FILE

# then use this paths text file to get each image metadata and write into SQL
# this will also take a while
./image_data_to_sql_paths.sh START_LINE PATHS_FILE DATABASE_FILE
# e.g
./image_data_to_sql_paths.sh 0 ~/data/paths/all_image_paths.txt ~/data/db/arXiv_db.sqlite

#+END_SRC
** Entry examples
*** Metadata
| 1038521 | hep-ph0107222 | 2001-07-20 | hep-ph          | ['Yang, Jian-Jun; ']                                       | Up and Down Quark Contributions...                   | We check the...                                                                 |                                                     |
| 1235851 |     0912.5313 | 2009-12-29 | math.CV math.AG | ['Catanese, Fabrizio; Oguiso, Keiji; Peternell, Thomas; '] | On volume preserving complex structures on real tori | A basic problem in the classification theory of compact complex manifolds is... | http://arxiv.org/licenses/nonexclusive-distrib/1.0/ |
| 1214856 |     1308.0124 | 2013-08-01 | hep-ph hep-th   | ['Rose, Luigi Delle; ']                                    | The Standard Model in a Weak Gravitational...        | The principal goal of the physics of the fundamental interactions is...         | http://arxiv.org/licenses/nonexclusive-distrib/1.   |
*** Images
| 4876126 |  cs0007002 | gouala05.eps     | 145239 | ./0007/cs0007002                                                                                          |  663 | 300 | PS  |
| 2209549 |  0906.0725 | belleescan_b.eps | 842045 | ./0906/0906.0725                                                                                          | 1450 | 725 | PS  |
| 6591348 | 1710.10269 | HAT-P-12.pdf     |  78468 | ./1710/1710.10269/figures/figures_from_umserve/chemistry/abundance_change_with_grid_parameter/metallicity |  566 | 406 | PDF |
** Querying
Once both tables have been created, it is then possible to perform SQL queries with a left join to pair the associated metadata with a given image. This allows us to create queries and perform analyses  that would not have been possible with only the bulk download: accessing the image data according to different metadata such as subject categories or date, performing searches, and analysing the image content of the dataset in various ways. The SQLite database provides a convenient and flexible way to perform these queries across the ~10 million images and ~1.5 million articles.

#+BEGIN_SRC SQLite
SELECT images.identifier, metadata.cat,count(images.identifier)    
FROM images
LEFT JOIN metadata ON images.identifier = metadata.identifier
GROUP BY images.identifier
ORDER BY count(images.identifier)
#+END_SRC

** Cleaning
The database required some cleaning after inserting rows for both metadata and images. This may be due to article revisions or the metadata harvester missing a few entries. A small number of images did not have corresponding article metadata. To fix this we used the list of articles without metadata and queried the arXiv OAI server. We then inserted these rows into the metadata table.

We also searched the SQLite database for any entries with special characters such as '/' that caused errors in the file insertion. In each arXiv category pre-2007, we removed the forward slash as this could potentially cause problems in how this data is read (this can be added back in as required). From the list of entries with special characters, many of these images were duplicated within the source upload. We checked that the images were present in the dataset and then removed the rows from the SQLite database as well as the files from the dataset. For later searches, we also filtered out any images that have an X or Y dimension of 0 or NULL, indicating that they could not be ready by the Imagemagick identify command.

** Adding image metadata

Additional metadata can be procured from the individual files by accessing the Exif (Exchangeable image file format). Although this could be done at the same time as inserting rows for the images into the database table, we did this at a later stage.

#+BEGIN_SRC bash
python imagemeta_to_sql_threads.py ~/data/db/arxiv_db_images.sqlite3 ~/arXiv/src_all/
#+END_SRC

This uses the exiftool command to check each file for metadata relating to "software" or "creator". Depending on the file extension, different fields are accessed. These were determined through testing exiftool across a range of file formats and checking which fields might relate to the software used to create or process the images. See the following Python excerpt for corresponding fields:

#+BEGIN_SRC Python
n = filename.lower()

if n.endswith(('.eps', '.ps', 'pstex', '.epsf', '.epsi')):
    field = "Creator"
elif n.endswith(('.png')):
    field = "Software"
elif n.endswith(('.pdf')):
    field = "Creator"
elif n.endswith(('.jpg', 'jpeg')):
    field = "Software"
elif n.endswith(('.gif')):
    field = "Comment"
elif n.endswith(('.svg')):
    field = "Desc"
#+END_SRC


