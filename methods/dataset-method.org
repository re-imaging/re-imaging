* Re-imaging the Empirical
Working with arXiv source dataset for image analysis, processing, and machine learning.

Process for downloading, extracting, unarchiving, and organising data for usage in machine learning applications as well as data mining.

** Step 1a: Download all source files from the arXiv Amazon Web Services (AWS) data storage using requestor pays.
- Information provided by arXiv: https://arxiv.org/help/bulk_data_s3
- At the time of downloading in January-February 2019, this equated to approximately 1 terabyte of data in the form of 2150 compressed tar archive files
- Downloading from UNSW in Australia, this download via AWS cost ~AU$150
- Download took several days
- Used s3cmd (https://s3tools.org/s3cmd and https://github.com/s3tools/s3cmd) for automated downloads using the --sync function (NB: turning off MD5 hash checking greatly speeds up process, but use with caution)

#+BEGIN_SRC bash
# first set up s3cmd with username and password

# testing - list files
s3cmd ls --requester-pays s3://arxiv/src/ 2>&1 | tee ~/s3cmd_log.txt

# get manifests
s3cmd get --requester-pays s3://arxiv/src/arXiv_src_manifest.xml
s3cmd get --requester-pays s3://arxiv/pdf/arXiv_pdf_manifest.xml

# NOTE: sync commands take a long time to begin due to checking md5s etc.
# may not respond or output any text for some time

# dry run
s3cmd sync --requester-pays -n s3://arxiv/src/ /mnt/hd-4tb/arXiv/src/

# sync with md5 check -- !!! this takes a long time !!!
s3cmd sync --requester-pays -v s3://arxiv/src/ /mnt/hd-4tb/arXiv/src/ 2>&1 | tee ~/s3cmd_log_checkmd5.txt
# sync without check and skipping existing
s3cmd sync --requester-pays --skip-existing --no-check-md5 -v s3://arxiv/src/ /mnt/hd-4tb/arXiv/src/ 2>&1 | tee ~/s3cmd_log.txt
#+END_SRC

Totals:
- 2150 compressed tar archive files

** Step 1b: Download OAI metadata
- arXiv maintains metadata for articles via the Open Access Initiative v2.0 protocol. This allows users to query the server to retrieve metadata relating to a particular time period or set (category, e.g. math)
- As we require the entire metadata database, we used a harvesting tool to request the metadata for the entire time range, which is downloaded incrementally in blocks of approximately 10,000 items
- Initial attempt at downloading metadata used oai-harvest (https://github.com/bloomonkey/oai-harvest). While simple to setup and use, the download only produced approximately 1.1 million records, which is much less than the expected 1.5 million records. The tool also downloaded items as individual xml files that each hold one record only, which has proven somewhat unwieldy
- Second attempt used metha (https://github.com/miku/metha). Metha is a command line harvester that incrementally caches a particular endpoint. This attempt produced 1506176 (1.5m) unique records

#+BEGIN_SRC bash

# first install metha using instructions from https://github.com/miku/metha

# check metha folder -- might not give output if not previously run
metha-sync -dir http://export.arxiv.org/oai2

# run harvester
metha-sync -format arXiv http://export.arxiv.org/oai2

# run harvester with date constraints
metha-sync -format arXiv -from 2019-01-01 http://export.arxiv.org/oai2
#+END_SRC

Totals:
- 1506176 OAI records across 1578 xml files

** Step 2: Unarchive and decompress
- Steps 2 and 3 were accomplished by using a number of single line bash commands that iteratively decompressed, unarchived, renamed, and organised the data, as well as extracting images and text from PDF documents.
- See document arxiv_extract.sh for annotated code
- Recommended to run each step individually and inspect for errors
- Extracting and unarchiving will take a long time
- During this process, images and text are extracted from each PDF file using pdf-images and pdftotext (this can be omitted, see below)

See arxiv_extract.sh - some code reproduced here:

#+BEGIN_SRC bash
# after downloading all arXiv tars and placing them in ~/arXiv/src

# change into directory
cd ~
mkdir arXiv
cd ~/arXiv/
mkdir src_all

# for each archive, decompress into a specific folder
for i in src/*; do tar xvf $i -C src_all/; done

# change directory - remaining commands are done from here
cd ~/arXiv/src_all

# move all pdf files to their own folder
find . -maxdepth 2 -name "*.pdf" -print -exec sh -c 'mkdir "${1%.*}" ; mv "$1" "${1%.*}" ' _ {} \;

# do the extraction here as it will operate only on the papers that were given only as pdf
# extract all images from pdf files
# find . -maxdepth 3 -name "*.pdf" -print -exec sh -c 'pdfimages -png "${1}" "${1}_image" ' _ {} \;

# extract text from pdf files
find . -name "*.pdf" -print -exec sh -c 'pdftotext "${1}" "${1%.*}_get.txt" ' _ {} \;

# for each archive within each subfolder
# find all gz tars, extract, and then delete the gz files
for d in *; do cd "$d" && for f in *.gz; do tar xvfz "$f" --one-top-level && rm "$f"; done; cd ..; done

# note that some of the archives are gz only and not tar
# seems to be because they only contain one file
# so for these we use gunzip which neatly replaces each .gz with a text file
find . -name "*.gz" -exec gunzip -v -q {} \;

# and for each individual (tex) file, make a folder and move the item to that folder
# note this needs to do some trickery as many of these files don't have extensions and we can't make a folder of the same name
find . -maxdepth 2 -type f -print -exec sh -c 'mkdir "${1}_dir" ; mv "$1" "${1}.srconly"  ; mv "${1}.srconly" "${1}_dir" ; mv "${1}_dir" "$1"' _ {} \;

#+END_SRC

Totals:
- 1,476,538 total articles (by number of folders extracted)
- 114,132 PDF-only articles (no source provided)
- 324,101 source-only articles (single source file only, no images)

File organisation
*** Directory structure
**** arXiv
***** src_all
****** YYMM
- 1512
- 1601
- 1602
******* individual article folders
- 1804.04821
- 1804.04822
- 1804.04823
- 1804.04824
- 1804.04825
******** subfolders for additional code or figures etc.
- figures
- diagrams
- text

*** Filenames
- Each article in the source directory has its own folder named in the format of YYMM.XXXXX (or only 4 digits in the form of YYMM.XXXX for pre-2015). Articles prior to March 2007 (9107-0703) use the identifier archive.subjectclass/YYMMXXX e.g. math.GT/0309136
- For more information on arXiv identifiers, see https://arxiv.org/help/arxiv_identifier
- Image files are named according to the original filenames that were deposited to arXiv, as we are using the original source

** Step 3: Extract images and text from PDF documents (NB: this took place during Step 2, but is outlined separately here)
- Extract images and text from PDF documents
- This originally seemed like an important process, as there is a decent portion of the arXiv that was not submitted as source code
- 7.69% of all articles are submitted as PDF only
- Attempted to use pdf-images to extract images, with varying success.
- Extracted over 27 million image files from PDFs
- Produced a very "dirty" dataset with a number of problems in the image files: A large number are "stripes" (images split into multiple horizontal bars) as well as lots of single symbols, strange transparency or inverted colours, and low resolution images
- Many of these are unusable. Some example montages of these problematic images can be found here: https://www.dropbox.com/sh/o6juhotbn9cih7w/AADWjarbKAs13U2fj_ZSKu1wa?dl=0
- Decision was made to ignore this part of the dataset and proceed with using only the images found in the source uploads. This will save time and effort in cleaning the data, as well as avoiding a number of pitfalls of having such a large and messy dataset, but at the cost of not having any images extracted from PDF files
- Each image extracted from a PDF was given the filename extension .pdf_image-XXX.png, so they can be ignored or conditionally operated upon at later stages of the process
- All PDF data was kept in case it would be required at a later stage in the project, and for posterity

Totals
- Total number of articles: 1,483,662
- Number of these that were PDF only: 114,132 (7.69% of total number of articles)
- 27,198,781 images extracted from PDFs

** Image totals
- Breakdown of the most common image formats. 
- There are more images than just these file extensions, but in uncommon formats, or in formats that are a bit tricky to work with (like metapost or xfig vector graphics languages), but the numbers of these are much smaller proportions of the dataset.

|----------+--------|
|      606 | .GIF   |
|      919 | .JPEG  |
|     1386 | .PDF   |
|     3425 | .epsf  |
|     5236 | .PS    |
|     7788 | .JPG   |
|    11256 | .PNG   |
|    12404 | .svg   |
|    15182 | .epsi  |
|    18496 | .gif   |
|    24190 | .pstex |
|    25141 | .EPS   |
|    26164 | .jpeg  |
|   450816 | .jpg   |
|   905970 | .ps    |
|  1090973 | .png   |
|  3299213 | .pdf   |
|  4202415 | .eps   |
|----------+--------|
| 10101580 | total  |
|----------+--------|


- Source uploads include a total of over 10 million images.
- These image formats are all relatively straightforward to work with and seem to give a good spread across different uses such as vector graphics (eps/svg), web (jpeg/gif), and print (ps)
- Mean average of 6.81 images per article
- Would be worthwhile to investigate and analyse proportion of images used across different categories and time
- Also important to keep looking for other strong tendencies or trends in the dataset e.g. is there something that has been missed through this process? By excluding PDF only articles are we missing a key part of the archive, or are these distributed uniformally?
** Step 4: Organise dataset
- Source dataset consists only of article source and image files, no metadata or data about the placement within arXiv
- OAI files consist only of metadata
- Place the data for both into SQLite database as an attempt to link this data and be able to analyse and label dataset
- Create SQLite database
- Parse OAI xml files and write relevant data into an SQLite table
- Create a table for individual images, iterate over all image files of relevant file extensions and insert a row into table for each
- Be able to query database for any images for a given article or metadata query, or matching metadata for a given image

#+BEGIN_SRC bash
# for full code, see file image_data_to_sql.sh
# code also reproduced in python, see image_data_to_sql.py

# find all relevant image files
find . -type f \( -iname "*.png" -o -iname "*.eps" -o -iname "*.pdf" -o -iname "*.ps" -o -iname "*.jpg" \
-o -iname "*.jpeg" -o -iname "*.pstex" -o -iname "*.gif" -o -iname "*.svg" -o -iname "*.epsf" \) \
-not -name "*pdf_image-*" | while read fullpath; do

  article="$(cut -d'/' -f3 <<< "$fullpath")"
  path="${fullpath%/*}"
  name="${fullpath##*/}"

  pdfext=$article

  pdfarticle="${article}.pdf"

  # check that the filename is not the same as the article ID, indicating a PDF of the article
  if [[ $name != $pdfarticle ]];
  then
    count=$((count+1))
    echo $count

    filesize=$(stat --printf="%s" "$fullpath")

    res="$(identify -ping -format "%w %h %m" "$fullpath")"

    x="$(cut -d' ' -f1 <<< "$res")"

    y="$(cut -d' ' -f2 <<< "$res")"

    imageformat="$(cut -d' ' -f3 <<< "$res")"

    # insert row into sqlite3
    sqlite3 /home/rte/data/db/arxiv_db_test.sqlite3 "INSERT INTO images \
    (identifier, filename, filesize, path, x, y, imageformat) \
    VALUES (\"$article\", \"$name\", \"$filesize\", \"$path\", \"$x\", \"$y\", \"$imageformat\");"
  fi
done

#+END_SRC

*** Further information
- In progress as of [2019-10-08 Tue]
