{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE with CUDA acceleration\n",
    "\n",
    "Run t-SNE over sets of images using the CannyLab tsne-cuda implementation: https://github.com/CannyLab/tsne-cuda\n",
    "\n",
    "Requires that feature vectors have already been extracted from images and these pickled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsnecuda import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# from pillow import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perp = 50\n",
    "bPCA = True\n",
    "num_iterations = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_folder = '/home/rte/re-imaging/visualisation/'\n",
    "\n",
    "paths = []\n",
    "\n",
    "for file in os.listdir(pickle_folder):\n",
    "    if file.endswith(\".pickle\") and file.startswith(\"features\"):\n",
    "        paths.append(os.path.join(pickle_folder, file))\n",
    "paths.sort()\n",
    "# print(paths)\n",
    "\n",
    "print(\"----- list of all paths: \")\n",
    "for p in paths:\n",
    "    print(p)\n",
    "print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop here\n",
    "\n",
    "for p in paths[:]:\n",
    "    print(p)\n",
    "    \n",
    "    category = p.split('_')[1]\n",
    "    year = p.split('_')[2]\n",
    "    print(\"category: \" + category)\n",
    "    print(\"year: \" + str(year))\n",
    "\n",
    "    with open(p, \"rb\") as read_file:\n",
    "        images, features = pickle.load(read_file)\n",
    "        read_file.close()\n",
    "    \n",
    "    # check that we still have the features and list of images\n",
    "    print(\"----- checking images and features -----\")\n",
    "    print(\"length of images: \" + str(len(images)))\n",
    "    print(\"length of features: \" + str(len(features)))\n",
    "    for img, f in list(zip(images, features))[0:5]:\n",
    "        print(\"image: %s, features: %0.2f,%0.2f,%0.2f,%0.2f... \"%(img, f[0], f[1], f[2], f[3]))\n",
    "    \n",
    "#     if len(images) >= 300:\n",
    "    if True:\n",
    "        features = np.array(features)\n",
    "        print(\"----- running pca across features -----\")\n",
    "        print(features.shape)\n",
    "        print(\"number of samples: \", np.size(features, 0))\n",
    "        pca = PCA(n_components=min(np.size(features,0), 300))\n",
    "        pca.fit(features)\n",
    "\n",
    "        pca_features = pca.transform(features)\n",
    "        \n",
    "        print(\"----- pca done -----\")\n",
    "        \n",
    "        print(\"----- running tSNE -----\")\n",
    "\n",
    "        X = np.array(pca_features)\n",
    "        print(X.shape)\n",
    "        tsne = TSNE(n_components=2, learning_rate=150, perplexity=perp, verbose=2, n_iter=num_iterations).fit_transform(X)\n",
    "        print(tsne.shape)\n",
    "        \n",
    "        print(\"----- tSNE done -----\")\n",
    "\n",
    "        # write pickle\n",
    "        print(\"writing tsne pickle\")\n",
    "        \n",
    "        ts = time.time()\n",
    "        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        \n",
    "        filename = \"tSNE_cuda_\" + category + \"_\" + year + \"_n\" + str(num_iterations) + \"_p\" + str(perp) + \"_\" + st\n",
    "        print(filename + \".pickle\")\n",
    "        \n",
    "        with open(filename + \".pickle\", \"wb\") as write_file:\n",
    "            pickle.dump([images, tsne], write_file)\n",
    "            write_file.close()\n",
    "            \n",
    "        # normalise points\n",
    "        tx, ty = tsne[:,0], tsne[:,1]\n",
    "        tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx))\n",
    "        ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))\n",
    "\n",
    "        width = 4000\n",
    "        height = 3000\n",
    "        max_dim = 100\n",
    "        \n",
    "        print(\"----- creating image from tiles -----\")\n",
    "\n",
    "        full_image = Image.new('RGBA', (width, height))\n",
    "        for img, x, y in zip(images, tx, ty):\n",
    "            tile = Image.open(img)\n",
    "            tw = tile.width\n",
    "            th = tile.height\n",
    "#             print(img)\n",
    "#             print(\"tile dimensions: x=\" + str(tile.width) + \" y=\" + str(tile.height))\n",
    "            if tw < 10000 and th < 10000:\n",
    "                rs = max(1, tw/max_dim, th/max_dim)\n",
    "                tile = tile.resize((int(tw/rs), int(th/rs)), Image.ANTIALIAS)\n",
    "                full_image.paste(tile, (int((width-max_dim)*x), int((height-max_dim)*y)), mask=tile.convert('RGBA'))\n",
    "#             else:\n",
    "#                 print(\"tile width or height too big?\")\n",
    "                \n",
    "        plt.figure(figsize = (16,12))\n",
    "        imshow(full_image)\n",
    "\n",
    "\n",
    "        print(\"saved file: \" + filename + \".png\")\n",
    "        full_image.save(filename + \".png\")\n",
    "        print(\"----- finished! file saved -----\")\n",
    "\n",
    "    else:\n",
    "        print(\"selected dataset has less than 300 items\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
