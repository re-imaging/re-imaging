{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import random\n",
    "import itertools\n",
    "import subprocess\n",
    "import os\n",
    "import shlex\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# from pillow import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import decode_predictions, preprocess_input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.VGG16(weights='imagenet', include_top=True)\n",
    "print(\"model loaded\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = image.load_img(path, target_size=model.input_shape[1:3])\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return img, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the feature extractor\n",
    "\n",
    "feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "print(\"feature extractor setup\")\n",
    "feat_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import the sqlite3 database and create a cursor\n",
    "db_path = \"/home/rte/data/db/arxiv_db_images.sqlite3\"\n",
    "db = sqlite3.connect(db_path)\n",
    "c = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that we can fetch the pragma for each table\n",
    "\n",
    "c.execute('PRAGMA TABLE_INFO({})'.format(\"metadata\"))\n",
    "info = c.fetchall()\n",
    "\n",
    "print(\"\\nColumn Info:\\nID, Name, Type, NotNull, DefaultVal, PrimaryKey\")\n",
    "for col in info:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = ('''\n",
    "    SELECT images.id, images.path, images.filename, images.identifier, metadata.cat\n",
    "    FROM images\n",
    "    LEFT JOIN metadata ON images.identifier = metadata.identifier\n",
    "    ORDER BY RANDOM()\n",
    "    LIMIT 1000\n",
    "    ''')\n",
    "\n",
    "c.execute(sql)\n",
    "rows = c.fetchall()\n",
    "print(len(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run convert on samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "\n",
    "targetSize = 512\n",
    "\n",
    "convert_path = \"/home/rte/data/images/random/1k/\"\n",
    "\n",
    "filepaths = []\n",
    "\n",
    "for row in rows:\n",
    "    path = row[1] + '/' + row[2]\n",
    "#     print(path)\n",
    "    filepaths.append(path.replace('./','/home/rte/arXiv/src_all/'))\n",
    "\n",
    "print(\"total number of filepaths: \" + str(len(filepaths)))\n",
    "\n",
    "# write list of image paths and IDs to file (for debugging purposes, mostly)\n",
    "\n",
    "fname = convert_path + \"filepaths.txt\"\n",
    "# print(fname)\n",
    "f = open(fname, \"w+\")\n",
    "for path, row in zip(filepaths, rows):\n",
    "    f.write(path + \",\" + str(row[0]) + \"\\n\")\n",
    "f.close()\n",
    "\n",
    "# arguments for convert\n",
    "# NOTE MODIFIED TO REMOVE \"^>\"\n",
    "arguments = shlex.split(\"-colorspace sRGB -background white -alpha off -resize \" + str(targetSize))\n",
    "# print(arguments)\n",
    "\n",
    "# call convert for each image path\n",
    "for row, f in zip(rows, filepaths):\n",
    "#     print(row)\n",
    "#     print(f)\n",
    "    outputname = [convert_path + str(row[0]) + \".jpg\"]\n",
    "\n",
    "#     print(\"calling convert\")\n",
    "    # call the montage command and parse list of files and arguments\n",
    "    convert_cmd = [\"convert\"] + [\"-density\"] + [\"300\"] + [f + \"[0]\"] + arguments + outputname\n",
    "#     print(convert_cmd)\n",
    "\n",
    "    result = subprocess.Popen(convert_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, err = result.communicate()\n",
    "#     print(out)\n",
    "    print(err)\n",
    "\n",
    "print(\"finished converting!\")\n",
    "end = time.time()\n",
    "print(\"time taken:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = \"/home/rte/data/images/random/1k/\"\n",
    "# current_path = convert_path\n",
    "\n",
    "image_extensions = ['.jpg', '.png', '.jpeg']   # case-insensitive (upper/lower doesn't matter)\n",
    "max_num_images = 100000\n",
    "\n",
    "images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(current_path) for f in filenames if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "num_x = len(images)\n",
    "print(\"keeping %d images to analyze\" % num_x)\n",
    "\n",
    "tic = time.clock()\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "features = []\n",
    "for i, image_path in enumerate(images):\n",
    "    if i % 500 == 0:\n",
    "        toc = time.clock()\n",
    "        elap = toc-tic;\n",
    "        print(\"analyzing image %d / %d. Time: %4.4f seconds.\" % (i, len(images),elap))\n",
    "        tic = time.clock()\n",
    "    img, x = load_image(image_path)\n",
    "\n",
    "#     feat = feat_extractor.predict(x)[0]\n",
    "#     features.append(feat)\n",
    "    \n",
    "    predictions = model.predict(x)\n",
    "    \n",
    "    for _, pred, prob in decode_predictions(predictions)[0]:\n",
    "        print(\"predicted %s with probability %0.3f\" % (pred, prob))\n",
    "    for image_pred in zip(pred, prob):\n",
    "        all_predictions.append(image_pred)\n",
    "        \n",
    "print('finished predicting class for %d images' % len(images))\n",
    "\n",
    "# write images, predictions to a pickle file\n",
    "\n",
    "f = \"classification_vgg_subset\" + \".pickle\"\n",
    "\n",
    "print(f)\n",
    "\n",
    "# WRITE\n",
    "with open(f, \"wb\") as write_file:\n",
    "    pickle.dump([images, features], write_file)\n",
    "    write_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
